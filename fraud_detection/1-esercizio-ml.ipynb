{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcde373f-6b6a-4a2e-b81f-6a2fabb92320",
   "metadata": {},
   "source": [
    "# INTRO\n",
    "The goal of this exercise is to build a machine learning model capable of distinguishing fraudulent customers from others.\n",
    "\n",
    "# DATASET\n",
    "The dataset consists of both clear-text columns and anonymized columns and represents a monthly snapshot of customers.\n",
    "Below is the explanation of the columns:\n",
    "\n",
    "- *data_rif*: end-of-month reference date\n",
    "- *userid*: customer ID\n",
    "- *age*: age\n",
    "- *profession*: profession\n",
    "- *region*: region of residence\n",
    "- *account_balance*: end-of-month account balance\n",
    "- *num_trx_cd*: number of debit card transactions executed during the month\n",
    "- *num_trx_cc*: number of credit card transactions executed during the month\n",
    "- *num_trx_cp*: number of prepaid card transactions executed during the month\n",
    "- *num_mov_conto*: number of current account movements during the month\n",
    "- *sum_mov_conto_pos*: sum of incoming current account transaction amounts during the month\n",
    "- *sum_mov_conto_neg*: sum of outgoing current account transaction amounts during the month\n",
    "- *num_prodotti*: number of products owned by the customer\n",
    "- *f2*, *f3*, *f4*, *f5*, *f6*, *f7*: anonymized behavioral features\n",
    "- *TARGET*: target variable indicating whether the customer committed fraud in the following months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b4d07",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "proxies = {\n",
    "    'http': 'http://inet1.gtm.corp.sanpaoloimi.com:9090/',\n",
    "    'https': 'http://inet1.gtm.corp.sanpaoloimi.com:9090/'\n",
    "}\n",
    "\n",
    "\n",
    "os.environ[\"http_proxy\"] = proxies[\"http\"]\n",
    "os.environ[\"https_proxy\"] = proxies[\"https\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b79722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m venv env\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e24c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import shap\n",
    "from shap import maskers\n",
    "from shap import TreeExplainer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer #needed for iterative imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, recall_score, matthews_corrcoef\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, ConfusionMatrixDisplay\n",
    "import kds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d68699-a88c-4e5d-9823-01c0e3ee6781",
   "metadata": {},
   "source": [
    "# Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410f097e-bd38-43c6-818b-8094cb07da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES = {\n",
    "    'data_rif': str,\n",
    "    'userid': np.int64,\n",
    "    'age':  np.int64,\n",
    "    'profession': str,\n",
    "    'region': str,\n",
    "    'account_balance': np.float64,\n",
    "    'num_trx_cd': np.float64,\n",
    "    'num_trx_cc': np.float64,\n",
    "    'num_trx_cp': np.float64,\n",
    "    'num_mov_conto': np.int32,\n",
    "    'sum_mov_conto_pos': np.int64,\n",
    "    'sum_mov_conto_neg': np.int64,\n",
    "    'num_prodotti': np.int64,\n",
    "    'f2': np.int64,\n",
    "    'f3': np.int64,\n",
    "    'f4': np.int64,\n",
    "    'f5': np.int64,\n",
    "    'f6': np.float64,\n",
    "    'f7': np.float64,\n",
    "    'TARGET': np.int64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f02ec0c-b963-4653-99ae-df461e64ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/frauds_dataset.csv\", sep=\"~\", dtype=DTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbe090c-1f2e-4399-9ade-e7949b9984b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_rif</th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>profession</th>\n",
       "      <th>region</th>\n",
       "      <th>account_balance</th>\n",
       "      <th>num_trx_cd</th>\n",
       "      <th>num_trx_cc</th>\n",
       "      <th>num_trx_cp</th>\n",
       "      <th>num_mov_conto</th>\n",
       "      <th>sum_mov_conto_pos</th>\n",
       "      <th>sum_mov_conto_neg</th>\n",
       "      <th>num_prodotti</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1000510</td>\n",
       "      <td>23</td>\n",
       "      <td>Lavoratore autonomo</td>\n",
       "      <td>TOSCANA</td>\n",
       "      <td>65627.799269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3590</td>\n",
       "      <td>-370</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>21.141686</td>\n",
       "      <td>0.268369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1001511</td>\n",
       "      <td>55</td>\n",
       "      <td>Lavoratore dipendente</td>\n",
       "      <td>BASILICATA</td>\n",
       "      <td>39335.109963</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>38.169452</td>\n",
       "      <td>0.672864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1001726</td>\n",
       "      <td>23</td>\n",
       "      <td>Lavoratore autonomo</td>\n",
       "      <td>PUGLIA</td>\n",
       "      <td>-37466.828926</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>636</td>\n",
       "      <td>-294</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>49</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>38.602380</td>\n",
       "      <td>0.126743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1002418</td>\n",
       "      <td>43</td>\n",
       "      <td>Studente</td>\n",
       "      <td>VALLE AOSTA</td>\n",
       "      <td>13864.880197</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1064</td>\n",
       "      <td>-1640</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>66</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>31.505413</td>\n",
       "      <td>2.081956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1002646</td>\n",
       "      <td>26</td>\n",
       "      <td>Studente</td>\n",
       "      <td>LOMBARDIA</td>\n",
       "      <td>-32625.910843</td>\n",
       "      <td>38.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>28</td>\n",
       "      <td>36.882651</td>\n",
       "      <td>0.210746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_rif   userid  age             profession       region  \\\n",
       "0  2022-07-31  1000510   23    Lavoratore autonomo      TOSCANA   \n",
       "1  2022-07-31  1001511   55  Lavoratore dipendente   BASILICATA   \n",
       "2  2022-07-31  1001726   23    Lavoratore autonomo       PUGLIA   \n",
       "3  2022-07-31  1002418   43               Studente  VALLE AOSTA   \n",
       "4  2022-07-31  1002646   26               Studente    LOMBARDIA   \n",
       "\n",
       "   account_balance  num_trx_cd  num_trx_cc  num_trx_cp  num_mov_conto  \\\n",
       "0     65627.799269         0.0         0.0         0.0             10   \n",
       "1     39335.109963         7.0         0.0         0.0              0   \n",
       "2    -37466.828926       148.0         0.0         0.0              2   \n",
       "3     13864.880197       215.0         0.0         0.0              8   \n",
       "4    -32625.910843        38.0        56.0         6.0              0   \n",
       "\n",
       "   sum_mov_conto_pos  sum_mov_conto_neg  num_prodotti   f2  f3  f4  f5  \\\n",
       "0               3590               -370             2   88  60   8  20   \n",
       "1                  0                  0             5   97  63  11  82   \n",
       "2                636               -294            10   90  49  31  71   \n",
       "3               1064              -1640             3   99  66  52  57   \n",
       "4                  0                  0             1  115  56  44  28   \n",
       "\n",
       "          f6        f7  TARGET  \n",
       "0  21.141686  0.268369       0  \n",
       "1  38.169452  0.672864       1  \n",
       "2  38.602380  0.126743       0  \n",
       "3  31.505413  2.081956       1  \n",
       "4  36.882651  0.210746       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08621c4-b38a-405c-9a26-69f17b9595bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24987, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a752b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.lower() for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9ec22-cd42-423a-b924-5a9d2d94dc0b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08e8ca-74dc-4ea3-a6d3-842619630478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f850a-86ec-4cae-aaba-fafee5dc73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentuale di target 1/0 per mese\n",
    "df_grouped = df.groupby(['data_rif','target']).agg(count_tr=('userid','count')).reset_index()\n",
    "tot = df.groupby(['data_rif']).agg(tot=('userid','count')).reset_index()\n",
    "df_grouped['perc_per_month'] = df_grouped.merge(tot, how='inner', on='data_rif').apply(lambda x: x['count_tr']/x['tot']*100,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5394e",
   "metadata": {},
   "source": [
    "The imbalance between the two classes is consistent across all months, which is useful for the GroupKFold strategy in cross-validation. The folds can be split by months (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f560de5-9b04-49f2-92e0-aa69e70eeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=df_grouped, x='data_rif', y='perc_per_month', hue='target')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()-0.5),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "\n",
    "plt.title('%Target 1vs0 per mese di riferimento')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('%')\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ea36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo qualche esempio di target 1 \n",
    "df.groupby(['userid']).agg(distinct = ('target','nunique')).reset_index().sort_values(by='distinct', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290caf8",
   "metadata": {},
   "source": [
    "There is a peculiarity in the data: the same userid at an earlier date has a greater age compared to later dates (e.g., 23 vs. 22 or even 23 vs. 44 years old). However, I assume this is due to the \"synthetic\" dataset, created to establish the relationship between fraud and age, which is later noticeable in the graphs below (i.e., customers with target 1 tend to be older).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c00434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['userid']==1623043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21410690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df['userid']==1160419]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b709c7",
   "metadata": {},
   "source": [
    "From the analysis of the numerical features, we observe that there will be missing values to handle in these three transaction-related features: *num_trx_cd*, *num_trx_cc*, *num_trx_cp*. Apart from this, no anomalies are noticed in the data (e.g., negative values in features that must necessarily be positive, such as age, *num_prodotti*, etc.). However, we can see that *sum_mov_conto_pos* and *sum_mov_conto_neg* are almost mirror images in their numerical characteristics (mean, std, max, and min), which indeed suggests a rather high correlation between these two variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['userid','target'], axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16083219",
   "metadata": {},
   "source": [
    "From the analysis of the categorical features, we observe that there will be missing values to handle in both *profession* and *region*. For further analysis, such as the distribution of the target across the various categories of these two features, please refer to the graphs below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['data_rif','userid','target'], axis=1).describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc81175",
   "metadata": {},
   "source": [
    "here the confirm of what said before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9611e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3a683",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "Removing data rif and userid since i won't use these features in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51f77",
   "metadata": {},
   "source": [
    "The analysis using KDE (normalized with respect to the number of values in each class) and countplot (percentage per values in the group) provides particularly interesting insights:\n",
    "\n",
    "* The most significant variables seem to be:\n",
    "    * *age*: the target variable = 1 appears to be associated with higher customer age values.\n",
    "    * *num_prodotti*: the same type of relationship as above.\n",
    "    * *fX*: all anonymized \"fX\" variables have different distributions between the two classes. For example, *f2* has a peak around 100 for target 1, which does not appear for target 0, and *f6* has a peak around 30. The most overlapped one is *f7*, which will be further investigated later with correlation analysis.\n",
    "\n",
    "* The distributions of *account_balance* are practically overlapping, so by itself, it does not provide much information for classification.\n",
    "* The number of transactions (*cc/cd/cp*) also has a similar distribution between the two classes; however, it's interesting to note that they are skewed to the right. This suggests that using the median instead of the mean might be preferable for replacing missing values. The distribution does not seem to indicate the presence of outliers that significantly affect the results; it looks more like a physiological distribution of values (for example, looking at the scatter plot for *num_trx_cd*, there are a couple of transactions above 400, but this is not too far from the most densely populated values).\n",
    "* The two variables, *sum_mov_conto_pos* and *sum_mov_conto_neg*, indeed have mirrored distributions (as could be expected a priori, and also as seen in the previous `describe()`), so it might be possible to remove one before training the model, as one implicitly contains the information of the other.\n",
    "\n",
    "The numerical range of values is very different, but below, tree-based models will be used, so no preprocessing of the numerical values will be done since these models are robust in this regard.\n",
    "\n",
    "* Regarding the categorical variables, we see that there do not seem to be significant variations in the 1/0 distribution with respect to *profession* (slightly higher percentage of 1 in the \"Unemployed\" class, but not very relevant), with slightly larger imbalances between regions. We decide to keep these types of variables after applying encoding, allowing the model to determine whether they are useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.drop(['userid','data_rif'],axis=1).select_dtypes(exclude=['object']):\n",
    "    if col == 'target':\n",
    "        continue\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.kdeplot(x=df[col], hue=df.target, common_norm=False)\n",
    "    plt.title(f'{col} kde')\n",
    "    \n",
    "for col in ['region','profession']:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    count_percentage = df.groupby(col)['target'].value_counts(normalize=True).mul(100).rename('percent').reset_index()\n",
    "\n",
    "    # Crea il countplot in percentuale\n",
    "    ax = sns.barplot(x=col, y='percent', hue='target', data=count_percentage)\n",
    "    \n",
    "    # Aggiungi le percentuali sulle barre\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height + 0.5, f'{height:.1f}%', ha='center')\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{col} countplot in %')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df.data_rif, y=df['num_trx_cd'])\n",
    "plt.title('Scatter Plot del Numero di Transazioni C.Debito')\n",
    "plt.xlabel('data rif')\n",
    "plt.ylabel('Numero di Transazioni')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48e9d8",
   "metadata": {},
   "source": [
    "In addition to univariate analysis, we will try to find relationships between variables that may assist the model: pairplot and correlation.\n",
    "\n",
    "* From the pairplot, the variables *num_prodotti* and *f4* stand out, confirming that higher values are associated with target 1. Additionally, there are relationships between *account_balance* and *f7*, which outline a certain range where frauds can be found (whereas previously *account_balance* alone seemed very overlapping between target 1 and 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.drop(['userid','data_rif','sum_mov_conto_neg'],axis=1), hue='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3281ad7",
   "metadata": {},
   "source": [
    "Below is an analysis of correlation and PPS (Predictive Power Score) between features and the target:\n",
    "\n",
    "* From the PPS analysis, the most useful features for prediction are *num_prodotti*, *age*, *f6*, *f2*, *f4*, *f5*, and *f7*, though with less predictive power (as noted from the KDE).\n",
    "\n",
    "* From the linear correlation analysis, the most useful features for prediction are *num_prodotti*, *age*, *f5*, *f6*, *f7*, *f4*, with less importance given to *f2* and *f3*.\n",
    "\n",
    "The results are generally consistent, but the different \"degrees\" of correlation/PPS might be due to the type of relationship between the feature and the target. Correlation identifies linear relationships, while PPS is based on decision trees.\n",
    "\n",
    "There are also high correlations (as expected) between *sum_mov_conto_pos* and *sum_mov_conto_neg* (negative correlation), and between these and *num_mov_conto*. In this case, it might be worth considering removing one of *sum_mov_conto_pos*/*sum_mov_conto_neg* and adding a feature that outlines the ratio between *num_mov_conto* and *sum_mov_conto_pos*, for example, `value_mov_medio = sum_mov_conto_pos / num_mov_conto`. I will try this later in the feature engineering section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# devo mettere il target categorico\n",
    "dfcat = df.drop(['userid','data_rif'],axis=1).copy()\n",
    "dfcat['target'] =dfcat['target'].astype('category')\n",
    "plt.figure(figsize=(8,6))\n",
    "predictors_df = pps.predictors(dfcat, y=\"target\")\n",
    "sns.barplot(data=predictors_df, x='x', y=\"ppscore\")\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "top_correlated_features = predictors_df.sort_values(by='ppscore',ascending=False).head(N)['x']\n",
    "print(\"Top Correlated Features with Target using ppscore:\")\n",
    "print(top_correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisi correlazione \n",
    "\n",
    "correlation_matrix = dfcat.corrwith(dfcat['target'])\n",
    "correlation_df = pd.DataFrame({'Correlation': correlation_matrix})\n",
    "\n",
    "sorteddf = correlation_df.abs().sort_values(by='Correlation', ascending=False)\n",
    "#print le N più correlate \n",
    "N=10\n",
    "top_correlated_features = sorteddf.head(N)\n",
    "print(\"Top Correlated Features with Target:\")\n",
    "print(top_correlated_features)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Top Features with Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcd033-c2b1-482a-934e-78026452f941",
   "metadata": {},
   "source": [
    "For the two numerical variables, we use Cramér's V test. If the value is close to 1, there is a relationship between *profession*/*region* and the target; otherwise, there is no significant relationship. For both columns, we do not find a significant relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af713916-d7d9-4137-8be1-15492083d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy = dfcat[['profession','region','target']].copy()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "dfcopy['profession_encoded'] = label_encoder.fit_transform(dfcopy['profession'])\n",
    "dfcopy['region_encoded'] = label_encoder.fit_transform(dfcopy['region'])\n",
    "\n",
    "for col in ['profession','region']:\n",
    "    contingency_table = pd.crosstab(dfcat[col], dfcat['target'])\n",
    "    chi2_stat, _, _, _ = chi2_contingency(contingency_table)\n",
    "    num_rows, num_cols = contingency_table.shape\n",
    "    cramer_v = np.sqrt(chi2_stat / (dfcat.shape[0] * (min(num_rows, num_cols) - 1)))\n",
    "    print(f\"Cramer's V {col}:\", cramer_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73476874",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdf188",
   "metadata": {},
   "source": [
    "### Missing\n",
    "\n",
    "The simplest method here would be to replace missing numerical values with the median, which I would use on larger datasets. However, since there are only 20k records, I will try the IterativeImputer, which uses other columns to impute the missing value.\n",
    "\n",
    "For categorical variables, you would need to encode the values first and then use IterativeImputer. For simplicity, I will use a SimpleImputer with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9444ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X = df.drop(['data_rif','userid'],axis=1).copy()  \n",
    "\n",
    "numeric_cols = ['num_trx_cd', 'num_trx_cc', 'num_trx_cp']\n",
    "categorical_cols = ['profession', 'region']\n",
    "\n",
    "\n",
    "numeric_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_imputer, numeric_cols),\n",
    "        ('categorical', categorical_imputer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X_imputed = model.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape pre imputation: {}',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(numeric_cols+categorical_cols, axis=1), pd.DataFrame(X_imputed, columns=numeric_cols+categorical_cols)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape after imputation: {}',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537574db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af010f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_trx_cd','num_trx_cc','num_trx_cp']]=df[['num_trx_cd','num_trx_cc','num_trx_cp']].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2de49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_trx_cd','num_trx_cc','num_trx_cp']].describe() #controllando sopra non sono variate molto le statistiche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ef124",
   "metadata": {},
   "source": [
    "### Creating New Features\n",
    "\n",
    "I will create a variable that shows the difference between positive and negative account movements, and another that gives the average value of the transactions made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a029bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_conto'] = df.apply(lambda x: x['sum_mov_conto_pos']+x['sum_mov_conto_neg'], axis=1)\n",
    "df['var_per_tr'] = df.apply(lambda x: (x['sum_mov_conto_pos']+x['sum_mov_conto_neg'])/x['num_mov_conto'] if x['num_mov_conto'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165328ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,9:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['sum_mov_conto_pos','sum_mov_conto_neg','num_mov_conto'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff533c02-51c4-4ee9-b2c6-f28516bb4b59",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "I will try two tree-based models: a Random Forest to establish a baseline and an XGBoost to see how much improvement can be achieved with a more complex model. \n",
    "\n",
    "Target 1 is of primary interest, and it is hypothesized that finding frauds is more important than identifying non-frauds. Therefore, we will emphasize the metric that accounts for false negatives (recall) rather than false positives. We will also look at the AUC score and the Matthews correlation coefficient, which evaluates predictions comprehensively (including false positives, false negatives, true positives, and true negatives). The AUC score can be optimistic for highly imbalanced problems since it calculates the false positive rate -> FP / (FP + TN). If TNs are very high in number, the value tends to zero, even if FP might be high compared to TP.\n",
    "\n",
    "To create the training set and test set, I will split based on the reference date. This mimics a real-world scenario where predictions are made for the most recent month. I will remove *userid* which will not be given to the model, but keep *data_rif* to use in GroupKFold. I choose this type of cross-validation because it allows me to use one month at a time as the validation set, ensuring the model does not overfit and maintains consistent performance for each prediction month.\n",
    "\n",
    "Note: If the dataset had more months, say a year, I would prefer a TimeSeries cross-validation, training the model on the first 6 months with the 7th month as the validation set, then from months 1 to 7 with the 8th month as validation, and so on up to month 11, using the 12th month as the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[df.data_rif<df.data_rif.max()].drop('userid',axis=1).copy()\n",
    "test = df.loc[df.data_rif==df.data_rif.max()].drop('userid',axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6565cf-bbf3-4086-917c-5cbd3ea6e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc91f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data_rif.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462f0ea-f572-42b3-916d-26a92953cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data_rif.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train = train.drop('target', axis=1).copy(), train['target'].copy()\n",
    "X_test , y_test = test.drop('target',axis=1).copy(), test['target'].copy()\n",
    "\n",
    "#creo le col per predizione\n",
    "cols = list(X_train.columns)\n",
    "cols.remove('data_rif')\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c3dd6",
   "metadata": {},
   "source": [
    "compute scale pos weight that might be useful for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5789558",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positive_examples = np.sum(y_train == 1)\n",
    "num_negative_examples = np.sum(y_train == 0)\n",
    "\n",
    "scale_pos_weight = num_negative_examples / num_positive_examples\n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f2b64",
   "metadata": {},
   "source": [
    "## Cross validation - RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d96cbb",
   "metadata": {},
   "source": [
    "handle categorical features with CatBoost encoder in the Cross validation loop to avoid target leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = CatBoostEncoder(drop_invariant=False,\n",
    "    return_df=True,\n",
    "    handle_unknown='UNKNOWN',\n",
    "    handle_missing='MISSING',\n",
    "    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = np.zeros(len(X_train))\n",
    "preds = np.zeros(len(X_train))\n",
    "\n",
    "skf = GroupKFold(n_splits=5)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate(skf.split(X_train, y_train, groups=X_train['data_rif'])):\n",
    "    \n",
    "    month = X_train.iloc[idxV]['data_rif'].iloc[0]\n",
    "    print('Fold', i, 'validation date', month)\n",
    "    print('Rows of train =', len(idxT), 'Rows of holdout =', len(idxV))\n",
    "    \n",
    "    #gestisco categoriche\n",
    "    ce.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT])\n",
    "    cvtrain =  ce.transform(X_train[cols].iloc[idxT])\n",
    "    cvval = ce.transform(X_train[cols].iloc[idxV])\n",
    "    \n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "    rf.fit(cvtrain, y_train.iloc[idxT])\n",
    "    \n",
    "    \n",
    "    val_preds_proba = rf.predict_proba(cvval)[:, 1]\n",
    "    val_preds = rf.predict(cvval)\n",
    "    \n",
    "    \n",
    "    proba[idxV] += val_preds_proba\n",
    "    preds[idxV] += val_preds\n",
    "    \n",
    "    # fold - auc su validation set\n",
    "    aucscore = roc_auc_score(y_train.iloc[idxV], val_preds_proba)   \n",
    "    print('validation-auc:', aucscore)\n",
    "\n",
    "    print('-' * 30)\n",
    "\n",
    "# metriche complessive\n",
    "print('#' * 20)\n",
    "print('RF AUC=', roc_auc_score(y_train, proba))\n",
    "print('RF RECALL CV=', recall_score(y_train, preds))\n",
    "print('RF MCC CV=', matthews_corrcoef(y_train, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report CV:\")\n",
    "print(classification_report(y_train, preds))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_train, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['0','1'])\n",
    "\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533993c",
   "metadata": {},
   "source": [
    "## Cross validation - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = np.zeros(len(X_train))\n",
    "preds = np.zeros(len(X_train))\n",
    "\n",
    "skf = GroupKFold(n_splits=5)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate(skf.split(X_train, y_train, groups=X_train['data_rif']) ):\n",
    "    month = X_train.iloc[idxV]['data_rif'].iloc[0]\n",
    "    print('Fold',i,'validation date',month)\n",
    "    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "    \n",
    "    #gestisco categoriche\n",
    "    ce.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT])\n",
    "    cvtrain =  ce.transform(X_train[cols].iloc[idxT])\n",
    "    cvval = ce.transform(X_train[cols].iloc[idxV])\n",
    "    \n",
    "    modelxgboost = xgb.XGBClassifier(n_estimators=100, max_depth=6,\n",
    "                            early_stopping_rounds=10, eval_metric='auc', random_state=42)\n",
    "\n",
    "    modelxgboost.fit(cvtrain, y_train.iloc[idxT], \n",
    "            eval_set=[(cvval, y_train.iloc[idxV])],\n",
    "            verbose=1000)\n",
    "    \n",
    "    proba[idxV] += modelxgboost.predict_proba(cvval)[:,1]\n",
    "    preds[idxV] += modelxgboost.predict(cvval)\n",
    "\n",
    "print('#'*20)\n",
    "print ('XGB NO SCALE POS WEIGHT AUC=',roc_auc_score(y_train, proba))\n",
    "print ('XGB NO SCALE POS WEIGHT RECALL CV=',recall_score(y_train, preds))\n",
    "print ('XGB NO SCALE POS WEIGHT MCC CV=',matthews_corrcoef(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report CV:\")\n",
    "print(classification_report(y_train, preds))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_train, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['0','1'])\n",
    "\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2dcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = np.zeros(len(X_train))\n",
    "preds = np.zeros(len(X_train))\n",
    "\n",
    "skf = GroupKFold(n_splits=5)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate(skf.split(X_train, y_train, groups=X_train['data_rif']) ):\n",
    "    month = X_train.iloc[idxV]['data_rif'].iloc[0]\n",
    "    print('Fold',i,'validation date',month)\n",
    "    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "    \n",
    "    \n",
    "    #gestisco categoriche\n",
    "    ce.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT])\n",
    "    cvtrain =  ce.transform(X_train[cols].iloc[idxT])\n",
    "    cvval = ce.transform(X_train[cols].iloc[idxV])\n",
    "    \n",
    "    modelxgboost_spw = xgb.XGBClassifier(n_estimators=100, max_depth=6, early_stopping_rounds=10, eval_metric='auc',\n",
    "                            random_state=42, scale_pos_weight=scale_pos_weight)\n",
    "    \n",
    " \n",
    "    modelxgboost_spw.fit(cvtrain, y_train.iloc[idxT], \n",
    "            eval_set=[(cvval,y_train.iloc[idxV])],\n",
    "            verbose=1000)\n",
    "    \n",
    "    proba[idxV] += modelxgboost_spw.predict_proba(cvval)[:,1]\n",
    "    preds[idxV] += modelxgboost_spw.predict(cvval)\n",
    "\n",
    "print('#'*20)\n",
    "print ('XGB SCALE POS WEIGHT AUC=',roc_auc_score(y_train, proba))\n",
    "print ('XGB SCALE POS WEIGHT RECALL CV=',recall_score(y_train, preds))\n",
    "print ('XGB SCALE POS WEIGHT MCC CV=',matthews_corrcoef(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec90058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report CV:\")\n",
    "print(classification_report(y_train, preds))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_train, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['0','1'])\n",
    "\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6b661",
   "metadata": {},
   "source": [
    "From the cross-validation, it is clear that XGBoost performs better than Random Forest, so it is worth using. Additionally, the *scale_pos_weight* parameter improves recall performance without significantly worsening the overall performance (MCC from 90 to 88): this model is chosen for the final evaluation.\n",
    "\n",
    "Note: The values set for *n_estimators* and *max_depth* were manually configured (the same for all three models) and were left as is after seeing that the performance was satisfactory. An additional hyperparameter tuning step could be added to further refine the metrics if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36c5b0",
   "metadata": {},
   "source": [
    "# Training over all the dataset and evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = X_train[cols].copy()\n",
    "xtest = X_test[cols].copy()\n",
    "ytrain = y_train.copy()\n",
    "ytest =y_test.copy()\n",
    "\n",
    "\n",
    "\n",
    "print(len(xtrain),len(ytrain))\n",
    "print(len(xtest), len(ytest))\n",
    "\n",
    "#fitto ce su tutto il dataset ora\n",
    "ce.fit(xtrain, ytrain)\n",
    "xtrain =  ce.transform(xtrain)\n",
    "xtest = ce.transform(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators=100, max_depth=6, eval_metric='auc',\n",
    "                        scale_pos_weight=scale_pos_weight,  random_state=42)\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d18f27-f161-4b1e-9190-1a134bce38f0",
   "metadata": {},
   "source": [
    "# Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783761f-ee63-42de-aa2d-56bbb5e7d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'TEST SET EVAL')\n",
    "\n",
    "\n",
    "y_pred = clf.predict(xtest)\n",
    "y_proba = clf.predict_proba(xtest)[:,1]\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(ytest, y_pred))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(ytest, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['0','1'])\n",
    "\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "roc_auc = roc_auc_score(ytest, y_proba)\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "mcc = matthews_corrcoef(ytest, y_pred)\n",
    "print(f\"MCC: {mcc}\")\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(ytest, y_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "precision, recall, thresholds_pr = precision_recall_curve(ytest, y_proba)\n",
    "\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(kds.metrics.decile_table(ytest, y_proba))\n",
    "\n",
    "kds.metrics.plot_cumulative_gain(ytest, y_proba)\n",
    "plt.show()\n",
    "plt.close()\n",
    "kds.metrics.plot_lift(ytest, y_proba)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46c70a",
   "metadata": {},
   "source": [
    "**Final Considerations**\n",
    "\n",
    "1. The performance seems to remain strong on the test set, comparable to what was found during cross-validation.\n",
    "2. The recall on the test set is very high, and at the same time, precision is not significantly affected. This appears to be an acceptable trade-off (the PR curve seems to confirm this). If 38 false negatives are deemed too many, one could increase the *scale_pos_weight* or conduct a study on the cutoff threshold.\n",
    "3. The lift is very high, as is the cumulative gain. In practice, with the second decile of predictions, we capture almost 100% of the frauds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffc396-863d-4fab-8e1b-4a046b839b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0e353-6441-44b4-a895-b2054cf4622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(clf, importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c412c2",
   "metadata": {},
   "source": [
    "As expected, we find the most important variables that were identified during the data exploration phase. The newly created variables do not seem to have made a significant contribution, so it might be worth trying to train a model using only the original variables.\n",
    "\n",
    "The SHAP plot indicates which features are most useful for the model's predictions and how these predictions are influenced by the input values of these variables. In this case, higher values of *num_prodotti* lead to higher predicted probabilities of fraud, similarly for *age*, *f7*, and *f6* (though the results for some of these variables are a bit more uncertain).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f75572",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = maskers.Independent(xtrain, 1000) \n",
    "exp = TreeExplainer(clf, background)\n",
    "sv = exp.shap_values(xtest)\n",
    "shap.summary_plot(sv, xtest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
